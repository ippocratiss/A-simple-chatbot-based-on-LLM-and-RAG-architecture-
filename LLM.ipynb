{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aea447-9e99-4ed5-ada0-424764787da6",
   "metadata": {},
   "source": [
    "#### Load the required functionality from the 'packages.py' and 'model.py' files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61890f31-9600-46ea-a119-fd7ca4b37c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saltas/.conda/envs/interactive_env2024_04/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-22 18:00:54.635407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from packages import * \n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4a161-3d2e-4f51-8fce-28331ac397a3",
   "metadata": {},
   "source": [
    "#### Rietrieval Augmentation: Define context's input, embedding and database construction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988efbe-a4bd-49b0-966f-ba545f074079",
   "metadata": {},
   "source": [
    "- The context file used to apply the RAG support to the LLM is not provided as it containts institutional data not to be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a65fe5-38c0-42b7-b295-073c625240f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path and file with the context information to be used by the RAG search engine\n",
    "# when assisting the LLM with institute-specific information. \n",
    "# This file is not provided here because of copyright issues with the included information.\n",
    "path = \"/home/saltas/Project_LLM\"\n",
    "loader = TextLoader(path + \"/Output2.txt\")\n",
    "\n",
    "# Loads the text file with the context and divides it into chunks \n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=10, add_start_index=True)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Define how to construct the embedings of the context file and the data base algorithm\n",
    "# I use the FAISS algorithm for the data base \n",
    "embeddings = HuggingFaceEmbeddings() \n",
    "db = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cc072-d416-4321-8ce2-0d5fb29b51f6",
   "metadata": {},
   "source": [
    "#### Define the LLM chain model to answer context-based questions (trained on internal wiki data, etc)\n",
    "##### Notes: \n",
    " -  The context file used to apply the RAG support to the LLM is not provided as it containts institutional data not to be shared.\n",
    " - The LLM model is defined in the 'model.py' file.\n",
    " - The temperature option can be tweaked accordingly. \n",
    " - For the moment the user input is an one-time input, cell needs to be re-run for new input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa8f03d-d1a6-4f21-a422-6a125990c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM chain: model + tokenizer + options\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    "                                                )\n",
    "\n",
    "## Define the prompt template for the LLM. This will provide to it generic instructions\n",
    "## and the relevant context derived from the RAG process, to help it answer the question.\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on context knowledge. Do not make up new information or things which are not based on context. \n",
    "Here is context:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Compile eveything into a LLM pipeline. \n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfecfee-5f6d-4ffb-ad95-76f02afabf63",
   "metadata": {},
   "source": [
    "#### Define the user input and the print out of an answer. \n",
    "##### Note: The contextual information to help the LLM is found through the database defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41957301-df42-40c9-ac50-845db7ba3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please ask me a question...  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No question? That's fine.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings \n",
    "\n",
    "question = input(\"Please ask me a question... \") # Define user prompt \n",
    "\n",
    "if question: \n",
    "    \n",
    "    retriever = db.as_retriever()\n",
    "    docs = retriever.invoke(question)\n",
    "    context = docs[0].page_content\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": lambda x: context, \"question\": RunnablePassthrough()} | llm_chain\n",
    "    )\n",
    "    \n",
    "    blah = rag_chain.invoke(question)\n",
    "    blah_answer = blah['text']\n",
    "    blah_answer_only = blah_answer.split(\"[/INST]\",1)[1]\n",
    "    \n",
    "    print(blah_answer_only)\n",
    "else:\n",
    "    print(\"No question? That's fine.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac30041-e06b-4161-85a5-caa31e5566d2",
   "metadata": {},
   "source": [
    "#### This is a different application of the LLM. One provides a pre-print arXiv number (https://arxiv.org/), the code loads the pdf article from online, and the network provides a summary of the paper.\n",
    "\n",
    "##### Notes:\n",
    "- The code has not been tested on many papers yet. It can happen that sometimes the output is erroneous. This is a matetr of fine-tuning.\n",
    "- The summary is provided based on the first few pages of the article. I do not include all the article to avoid overloading the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c385dc-3b21-4db1-881f-ecafeb872f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide me the arXiv number of the paper (e.g 2405.12685) and press Enter 2405.12685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDocument loaded OK.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "arXiv_number = input(\"Provide me the arXiv number of the paper (e.g 2405.12685) and press Enter\")\n",
    "text_splitter = CharacterTextSplitter()\n",
    "pages = load_doc('https://arxiv.org/pdf/' + str(arXiv_number))\n",
    "\n",
    "pages_all = pages[0].page_content # all pages \n",
    "max_pages = 4 #len(pages) # max number of pages. \n",
    "# I set the max_pages to the first few to avoid overloading the model. \n",
    "# This should naturally include abstract + introduction of the paper.\n",
    "\n",
    "for i in range(1,max_pages): # construct a big string with all the pages \n",
    "    pages_all = pages_all + pages[i].page_content\n",
    "docs = pages_all\n",
    "docs = doc(pages_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21fd3e5-dc78-443a-9e8e-2e7df5104bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis is a bullet-point summary of the paper:\u001b[0m\n",
      " \n",
      "\n",
      "* The paper discusses the effective field theory formulation of gravity.\n",
      "* It critically assesses\n",
      "the common practice of using local field redefinitions in order to simplify the dynamics.\n",
      "* The\n",
      "paper reviews the corrections related to local effective interactions containing up to six\n",
      "derivatives.\n",
      "* It identifies the parameterized post-Newtonian (PPN) order at which a given higher-\n",
      "derivative term starts to contribute also at the non-linear level.\n",
      "* The paper is closely related to\n",
      "recent efforts to construct black hole spacetimes in the framework of effective field theory.\n",
      "* The\n",
      "paper uses the Einstein-Hilbert action as a starting point and organizes the expansion of the\n",
      "gravitational action in terms of derivatives.\n",
      "* It identifies the relevant terms in the derivative\n",
      "expansion of the gravitational action.\n",
      "* The paper adopts a specific choice for the set of\n",
      "independent interaction terms.\n",
      "* The paper groups the terms according to their physics implications.\n",
      "* The paper sets out the relevant notation and definitions.\n"
     ]
    }
   ],
   "source": [
    "#### As usual, define the prompt template.\n",
    "prompt_template = \"\"\" Write a concise summary in bullet points of the following text. \n",
    "Try to ignore equations, images, and refereces/citations:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY IN BULLET POINTS: \"\"\"\n",
    "#### \n",
    "\n",
    "summary_prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=[\"text\"])\n",
    "#\n",
    "chain = load_summarize_chain(mistral_llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=summary_prompt)\n",
    "#\n",
    "output_summary = chain.run(docs) # apply the chain on the loaded document \n",
    "#\n",
    "answer_only = output_summary.split(\"SUMMARY IN BULLET POINTS:\",1)[1] # extract only the summary, not document.\n",
    "#\n",
    "wrap = textwrap.fill(answer_only, \n",
    "                             width=100,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "# print out the result\n",
    "print(bf('This is a bullet-point summary of the paper:'))\n",
    "print(wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115c101-1e82-4daa-99ec-aae261faeaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
